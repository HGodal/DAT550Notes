\section{Addressing Overfitting}
Multiple strategies can be used to check if overfitting occurs:
\begin{itemize}
    \item Plot the hypothesis to check for overfitting. Might not always work.
    \item Reduce the number of features
    \item Regularization
\end{itemize}

\section{Regularization}

Regularization is the process of adding information in order to solve an 
ill-posed problem or to prevent overfitting. Normally in machine learning
regularization is done by introducing a regularization term $R(f)$ to the loss
function 

\begin{equation*}
    \min_f \sum_{i = 1}^n V(f(x_i), y_i) + \lambda R(f)
\end{equation*}

Where $V$ is the underlying loss function and $R(f)$ is typically chosen to 
impose a penalty on the complexity of $f$. Regularization in one way imposes
occam's razor on the solution, the principle that the simpler explanation 
(model) is usually the best. Another perspective is the Bayesian one where 
prior knowledge is applied to the model parameters.

\begin{itemize}
    \item Small values for parameters correspond to a simpler hypothesis (you effectively get rid of some of the terms).
    \item A simpler hypothesis is less prone to overfitting.
\end{itemize}

\subsection{Cost function optimalization}
We can penalize some of the higher-order $\theta$ parameters and make them very small:

\[
    \min\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}(x_i)-y_i\right)^2 + 1000\theta_3^2 + 1000\theta_4^2
\]

\subsection{Regularization on features}
Unlike in a polynomial, we don't know which parameters are the high-order terms. The solution for this is to penalize all the parameters.

By convention, we don't penalize $\theta_0$, but all other parameters, with the regularization parameter $\lambda$.

\[
    J(\theta) = \frac{1}{2m}\left[\sum_{i=1}^{m}\left(h_{\theta}(x_i)-y_i\right)^2 + \lambda\sum_{j=1}^{n}\theta_j^2\right]
\]

\subsection{Regularized Gradient Descent}
\[
    \theta_j := \theta_j - \alpha\left[\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x_i)-y_i\right)x_i^{(j)} + \frac{\lambda}{m}\theta_j\right]
\]

\[
    \theta_j := \theta_j\left(1-\alpha\frac{\lambda}{m}\right) - \alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x_i)-y_i\right)x_i^{(j)}
\]

Given a typical small learning rate $\lambda$ and a large number of terms $m$, we see that $\theta_j$ is multiplied by a value close to $1$ (around $0.95-0.99$).
